{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projeto_01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2TZVPjU5kjHPJ9j17ztdW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antuko7/DeepLearning/blob/main/Projeto/Projeto_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Image Captioning Projec to develop IA driver assitance **"
      ],
      "metadata": {
        "id": "7_9CpiDp9Eas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (8.0, 10.0)"
      ],
      "metadata": {
        "id": "aRu3QMCF2avR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/zips/train2017.zip\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!wget http://images.cocodataset.org/zips/test2017.zip\n",
        "\n",
        "!unzip train2017.zip\n",
        "!unzip val2017.zip\n",
        "!unzip test2017.zip\n",
        "\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/image_info_test2017.zip\n",
        "\n",
        "!unzip annotations_trainval2017.zip\n",
        "!unzip stuff_annotations_trainval2017.zip\n",
        "!unzip image_info_test2017.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSGrc9uk-Mfd",
        "outputId": "e7e32754-8037-4aad-a3bc-e615631b41f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-22 20:58:37--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.224.49\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.224.49|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘train2017.zip’\n",
            "\n",
            "train2017.zip         6%[>                   ]   1.19G  67.6MB/s    eta 3m 56s ^C\n",
            "--2022-06-22 20:58:55--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.93.68\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.93.68|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘val2017.zip’\n",
            "\n",
            "val2017.zip          12%[=>                  ]  98.70M  98.4MB/s               ^C\n",
            "--2022-06-22 20:58:56--  http://images.cocodataset.org/zips/test2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.93.68\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.93.68|:80... connected.\n",
            "HTTP request sent, awaiting response... ^C\n",
            "Archive:  train2017.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of train2017.zip or\n",
            "        train2017.zip.zip, and cannot find train2017.zip.ZIP, period.\n",
            "Archive:  val2017.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of val2017.zip or\n",
            "        val2017.zip.zip, and cannot find val2017.zip.ZIP, period.\n",
            "unzip:  cannot find or open test2017.zip, test2017.zip.zip or test2017.zip.ZIP.\n",
            "--2022-06-22 20:58:56--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.93.68\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.93.68|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  96.7MB/s    in 2.5s    \n",
            "\n",
            "2022-06-22 20:58:59 (96.7 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n",
            "--2022-06-22 20:58:59--  http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.133.235\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.133.235|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1148688564 (1.1G) [application/zip]\n",
            "Saving to: ‘stuff_annotations_trainval2017.zip’\n",
            "\n",
            "val2017.zip          61%[===========>        ] 671.39M   104MB/s    eta 5s     "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "from typing import Any, Callable, Optional, Tuple, List\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "\n",
        "\n",
        "class CocoDetection(VisionDataset):\n",
        "    \"\"\"`MS Coco Detection <https://cocodataset.org/#detection-2016>`_ Dataset.\n",
        "\n",
        "    It requires the `COCO API to be installed <https://github.com/pdollar/coco/tree/master/PythonAPI>`_.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory where images are downloaded to.\n",
        "        annFile (string): Path to json annotation file.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
        "            and returns a transformed version.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        annFile: str,\n",
        "        transform: Optional[Callable] = None,\n",
        "        target_transform: Optional[Callable] = None,\n",
        "        transforms: Optional[Callable] = None,\n",
        "    ) -> None:\n",
        "        super().__init__(root, transforms, transform, target_transform)\n",
        "        from pycocotools.coco import COCO\n",
        "\n",
        "        self.coco = COCO(annFile)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "    def _load_image(self, id: int) -> Image.Image:\n",
        "        path = self.coco.loadImgs(id)[0][\"file_name\"]\n",
        "        return Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n",
        "\n",
        "    def _load_target(self, id: int) -> List[Any]:\n",
        "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        id = self.ids[index]\n",
        "        image = self._load_image(id)\n",
        "        target = self._load_target(id)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            image, target = self.transforms(image, target)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.ids)\n",
        "\n",
        "\n",
        "\n",
        "class CocoCaptions(CocoDetection):\n",
        "    \"\"\"`MS Coco Captions <https://cocodataset.org/#captions-2015>`_ Dataset.\n",
        "\n",
        "    It requires the `COCO API to be installed <https://github.com/pdollar/coco/tree/master/PythonAPI>`_.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory where images are downloaded to.\n",
        "        annFile (string): Path to json annotation file.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
        "            and returns a transformed version.\n",
        "\n",
        "    Example:\n",
        "\n",
        "        .. code:: python\n",
        "\n",
        "            import torchvision.datasets as dset\n",
        "            import torchvision.transforms as transforms\n",
        "            cap = dset.CocoCaptions(root = 'dir where images are',\n",
        "                                    annFile = 'json annotation file',\n",
        "                                    transform=transforms.ToTensor())\n",
        "\n",
        "            print('Number of samples: ', len(cap))\n",
        "            img, target = cap[3] # load 4th sample\n",
        "\n",
        "            print(\"Image Size: \", img.size())\n",
        "            print(target)\n",
        "\n",
        "        Output: ::\n",
        "\n",
        "            Number of samples: 82783\n",
        "            Image Size: (3L, 427L, 640L)\n",
        "            [u'A plane emitting smoke stream flying over a mountain.',\n",
        "            u'A plane darts across a bright blue sky behind a mountain covered in snow',\n",
        "            u'A plane leaves a contrail above the snowy mountain top.',\n",
        "            u'A mountain that has a plane flying overheard in the distance.',\n",
        "            u'A mountain view with a plume of smoke in the background']\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def _load_target(self, id: int) -> List[str]:\n",
        "        return [ann[\"caption\"] for ann in super()._load_target(id)]"
      ],
      "metadata": {
        "id": "UbxlsxwT9OKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CI16l0gII3n",
        "outputId": "906b877a-efaa-44ce-8b19-ccb21a890cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  train2017.zip  val2017  val2017.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = data.COCODetection('.',splits=['instances_val2017'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "FiEbjPZ9Hl9R",
        "outputId": "f4126cf4-06b9-485a-b511-17d4413ce201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7d3dd8837cf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOCODetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'instances_val2017'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    }
  ]
}